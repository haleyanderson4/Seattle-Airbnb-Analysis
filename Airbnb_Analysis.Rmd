---
title: "Final Project"
author: "Haley Anderson,  Allison Merrill, Annie Zhu, Karolina Michalewska"
subtitle: "MGSC 310, Fall 2019, Professor Hersh"
output:
  pdf_document: default
  word_document: default
  html_notebook: default
---

```{r setup, include=FALSE}
library(knitr)

# set seed to your own favorite number
set.seed(1818)
options(width=70)

# general rchunk code options
opts_chunk$set(tidy.opts=list(width.wrap=50),tidy=TRUE, size = "vsmall")
opts_chunk$set(message = FALSE,
               warning = FALSE,
               cache = TRUE,
               autodep = TRUE,
               cache.comments = FALSE,
               collapse = TRUE,
               fig.width = 5,  
               fig.height = 4,
               fig.align='center')

# install packages
#install.packages('wordcloud2')
#install.packages('slam')
#install.packages("tm")  # for text mining
#install.packages("SnowballC") # for text stemming
#install.packages("wordcloud") # word-cloud generator 
#install.packages("RColorBrewer") # color palettes
#install.packages('randomForest')
#install.packages('ranger')
#install.packages('randomForestExplainer')
#install.packages('stargazer')
#install.packages('tokenizer')
```
## Data Cleaning
```{r}
library('tidyverse')
library('tokenizers')
library('magrittr')

listings <- read_csv("listings_clean.csv")
summary(listings)

# Data cleaning, removing unecessary variables 
listings <- listings %>% filter(city=="Seattle", state=="WA")

# need to convert from factors to strings
listings %<>% mutate(amenities = as.character(amenities))

# from strings to tokens
head(tokenize_words(listings$amenities))

head(listings$amenities)

# create a variable to hold the separated amenities
listings %<>%
  mutate(amenities_separated = NA) %>%
  select(id,amenities, amenities_separated, everything() )


# clean each of the rows of listings
for (i in 1:nrow(listings)){
  amenities <- unlist(str_split(listings$amenities[i], pattern = ","))
  amenities_vec <- str_replace_all(amenities,pattern = "\"", replacement = "")
  amenities_vec2 <- str_replace_all(amenities_vec,pattern = "\\(|\\)", replacement = "")
  listings$amenities_separated[i] <- paste(amenities_vec2, collapse = ", ")
}

head(listings$amenities_separated)

# create column of unique listings of amenities
total_amenities = unique(unlist(str_split(listings$amenities_separated, pattern = ",") ))
total_amenities = str_replace_all(total_amenities, pattern = "^ ", replacement="")
total_amenities = str_replace_all(total_amenities, pattern = "\\(|\\)", replacement="")


listings_wide = listings

# loop through all amenities in DF and flag = 1 if amenities match the listed one  
for (i in 1:length(total_amenities)){
  listings_wide[,total_amenities[i]] <- 
    str_count(listings_wide$amenities_separated, pattern = total_amenities[i])
}
listings_wide[,total_amenities]

listings_wide %<>%
  select(id, amenities, amenities_separated, total_amenities, everything())

colnames(listings_wide)[4] <- "Cable_TV"
colnames(listings_wide)[5] <- "Wireless_Internet"
colnames(listings_wide)[6] <- "Air_Conditioning"
colnames(listings_wide)[9] <- "Family_Kid_Friendly"
colnames(listings_wide)[12] <- "Free_Parking_on_Premises"
colnames(listings_wide)[13] <- "Buzzer_Wireless_Intercom"
colnames(listings_wide)[14] <- "Smoke_Detector"
colnames(listings_wide)[15] <- "CO_Detector"
colnames(listings_wide)[16] <- "First_Aid_Kit"
colnames(listings_wide)[17] <- "Safety_Card"
colnames(listings_wide)[18] <- "Fire_Extinguisher"
colnames(listings_wide)[20] <- "Pets_Allowed"
colnames(listings_wide)[21] <- "Pets_live_on_this_property"
colnames(listings_wide)[24] <- "Hot_Tub"
colnames(listings_wide)[25] <- "Indoor_Fireplace"
colnames(listings_wide)[28] <- "TwentyfourHour_Checkinin"
colnames(listings_wide)[32] <- "Laptop_Friendly_Workspace"
colnames(listings_wide)[33] <- "Suitable_for_Events"
colnames(listings_wide)[34] <- "Elevator_in_Building"
colnames(listings_wide)[35] <- "Lock_on_Bedroom_Door"
colnames(listings_wide)[36] <- "Wheelchair_Accessible"
colnames(listings_wide)[40] <- "Smoking_Allowed"
colnames(listings_wide)[41] <- "Other_pets"
colnames(listings_wide)[43] <- "Washer_Dryer"

colnames(listings_wide)

listings_wide$Pool <- as.factor(as.character(listings_wide$Pool))
listings_wide$Cable_TV <- as.factor(as.character(listings_wide$Cable_TV))
listings_wide$Air_Conditioning <- as.factor(as.character(listings_wide$Air_Conditioning))
listings_wide$Heating <- as.factor(as.character(listings_wide$Heating))
listings_wide$Family_Kid_Friendly <- as.factor(as.character(listings_wide$Family_Kid_Friendly))
listings_wide$Washer <- as.factor(as.character(listings_wide$Washer))
listings_wide$Dryer <- as.factor(as.character(listings_wide$Dryer))
listings_wide$Buzzer_Wireless_Intercom <- as.factor(as.character(listings_wide$Buzzer_Wireless_Intercom))
listings_wide$First_Aid_Kit <- as.factor(as.character(listings_wide$First_Aid_Kit))
listings_wide$Pets_live_on_this_property <- as.factor(as.character(listings_wide$Pets_live_on_this_property))
listings_wide$Hangers <- as.factor(as.character(listings_wide$Hangers))
listings_wide$Laptop_Friendly_Workspace <- as.factor(as.character(listings_wide$Laptop_Friendly_Workspace))
listings_wide$Suitable_for_Events <- as.factor(as.character(listings_wide$Suitable_for_Events))
listings_wide$Lock_on_Bedroom_Door <- as.factor(as.character(listings_wide$Lock_on_Bedroom_Door))
listings_wide$Wheelchair_Accessible <- as.factor(as.character(listings_wide$Wheelchair_Accessible))
listings_wide$Smoking_Allowed <- as.factor(as.character(listings_wide$Smoking_Allowed))
listings_wide$Other_pets <- as.factor(as.character(listings_wide$Other_pets))
listings_wide$Doorman <- as.factor(as.character(listings_wide$Doorman))
listings_wide$Gym <- as.factor(as.character(listings_wide$Gym))
listings_wide$HairDryer <- as.factor(as.character(listings_wide$HairDryer))
listings_wide$Pets_Allowed <- as.factor(as.character(listings_wide$Pets_Allowed))
listings_wide$Dogs <- as.factor(as.character(listings_wide$Dogs))
listings_wide$Cats <- as.factor(as.character(listings_wide$Cats))
listings_wide$TwentyfourHour_Checkinin <- as.factor(as.character(listings_wide$TwentyfourHour_Checkinin))
listings_wide$Breakfast <- as.factor(as.character(listings_wide$Breakfast))
listings_wide$Fire_Extinguisher <- as.factor(as.character(listings_wide$Fire_Extinguisher))
listings_wide$Safety_Card <- as.factor(as.character(listings_wide$Safety_Card))
listings_wide$CO_Detector <- as.factor(as.character(listings_wide$CO_Detector))
listings_wide$Smoke_Detector <- as.factor(as.character(listings_wide$Smoke_Detector))
listings_wide$Free_Parking_on_Premises <- as.factor(as.character(listings_wide$Free_Parking_on_Premises))
listings_wide$Iron <- as.factor(as.character(listings_wide$Iron))
listings_wide$Elevator_in_Building <- as.factor(as.character(listings_wide$Elevator_in_Building))
listings_wide$Indoor_Fireplace <- as.factor(as.character(listings_wide$Indoor_Fireplace))
listings_wide$Essentials <- as.factor(as.character(listings_wide$Essentials))
listings_wide$Shampoo <- as.factor(as.character(listings_wide$Shampoo))
listings_wide$Washer_Dryer <- as.factor(as.character(listings_wide$Washer_Dryer))
listings_wide$Kitchen <- as.factor(as.character(listings_wide$Kitchen))
listings_wide$Wireless_Internet <- as.factor(as.character(listings_wide$Wireless_Internet))
listings_wide$require_guest_phone_verification <- as.factor(as.character(listings_wide$require_guest_phone_verification))
listings_wide$requires_license <- as.factor(as.character(listings_wide$requires_license))
listings_wide$require_guest_profile_picture <- as.factor(as.character(listings_wide$require_guest_profile_picture))
listings_wide$host_is_superhost <- as.factor(as.character(listings_wide$host_is_superhost))
listings_wide$property_type <- as.factor(as.character(listings_wide$property_type))
listings_wide$instant_bookable <- as.factor(as.character(listings_wide$instant_bookable))
listings_wide$host_has_profile_pic <- as.factor(as.character(listings_wide$host_has_profile_pic))
listings_wide$host_identity_verified <- as.factor(as.character(listings_wide$host_identity_verified))
listings_wide$room_type <- as.factor(as.character(listings_wide$room_type))
listings_wide$id <- as.factor(as.character(listings_wide$id))
listings_wide$experiences_offered <- as.factor(as.character(listings_wide$experiences_offered))
listings_wide$host_response_rate <- as.numeric(as.character(listings_wide$host_response_rate))
listings_wide$host_acceptance_rate <- as.numeric(as.character(listings_wide$host_acceptance_rate))
listings_wide$host_neighbourhood <- as.factor(as.character(listings_wide$host_neighbourhood))
listings_wide$neighbourhood <- as.factor(as.character(listings_wide$neighbourhood))
listings_wide$neighbourhood_cleansed <- as.factor(as.character(listings_wide$neighbourhood_cleansed))
listings_wide$zipcode <- as.factor(as.character(listings_wide$zipcode))
listings_wide$state <- as.factor(as.character(listings_wide$state))
listings_wide$bed_type <- as.factor(as.character(listings_wide$bed_type))
listings_wide$cancellation_policy <- as.factor(as.character(listings_wide$cancellation_policy))
listings_wide$first_review <- as.Date(as.character(listings_wide$first_review,"%Y-%m-%d"))
listings_wide$last_review <- as.Date(as.character(listings_wide$last_review,"%Y-%m-%d"))
listings_wide$host_since <- as.Date(as.character(listings_wide$host_since,"%Y-%m-%d"))
glimpse(listings_wide)


listings_clean <- listings_wide[,-c(1:5,38,44:47,56,59,60,62:63,81:82,99,105,107)]

# Remove listings that have never been stayed at
listings_clean <- listings_clean %>% drop_na(host_acceptance_rate)
listings_clean <- listings_clean %>% filter(host_acceptance_rate!="N/A")

listings_clean <- listings_clean %>% mutate(
  price = as.numeric(gsub("\\$", "", as.character(listings_clean$price))),
  security_deposit = as.numeric(gsub("\\$", "", as.character(listings_clean$security_deposit))),
  cleaning_fee = as.numeric(gsub("\\$", "", as.character(listings_clean$cleaning_fee))),
  extra_people = as.numeric(gsub("\\$", "", as.character(listings_clean$extra_people))),
)
```

## Summary Statistics
```{r}
# Predicting the nightly price of an Airbnb listing

num_listing <- nrow(listings_clean)
cat("Number of Listings in Seattle: ", num_listing, "\n")

house_percent <- (nrow(listings_clean %>% filter(property_type=="House")) /  num_listing)
cat("Percentage of Listings that are a House: ", house_percent, "\n")

apartment_percent <- (nrow(listings_clean %>% filter(property_type=="Apartment")) /  num_listing)
cat("Percentage of Listings that are an Apartment: ", apartment_percent, "\n")

print(house_percent + apartment_percent)

other_property_listings <- listings_clean %>% filter(property_type!="House", 
                                                     property_type!="Apartment")
#Some of the other property types are:
head(other_property_listings$property_type)

full_house_percent <- (nrow(listings_clean %>% filter(room_type=="Entire home/apt")) / num_listing)
cat("Percentage of Listings that are the Full House / Apartment: ", full_house_percent, "\n")

priv_room_percent <- (nrow(listings_clean %>% filter(room_type=="Private room")) / num_listing)
cat("Percentage of Listings that are a Private Room: ", priv_room_percent, "\n")

share_room_percent <- (nrow(listings_clean %>% filter(room_type=="Shared room")) / num_listing)
cat("Percentage of Listings that are a Shared Room: ", share_room_percent, "\n")

print(full_house_percent + priv_room_percent + share_room_percent)

avg_nightly_price <- mean(listings_clean$price, na.rm = TRUE)
median_nightly_price <- median(listings_clean$price, na.rm = TRUE)
cat("Average Nightly Price: ", avg_nightly_price, " ||  Median Nightly Price: ", median_nightly_price, "\n")

std_dev_nightly_price <- sd(listings_clean$price, na.rm = TRUE)
cat("Standard Deviation: ", std_dev_nightly_price, "\n")

max_nightly_price <- max(listings_clean$price, na.rm = TRUE)
min_nightly_price <- min(listings_clean$price, na.rm = TRUE)
cat("Minimum Nightly Price: ", min_nightly_price, " ||  Maximum Nightly Price: ", max_nightly_price, "\n")

avg_num_reviews <- mean(listings_clean$number_of_reviews, na.rm = TRUE)
avg_rating <- mean(listings_clean$review_scores_rating, na.rm = TRUE)
cat("Average number of Reviews: ", avg_num_reviews, " ||  Average Rating out of 100: ", avg_rating, "\n")

std_dev_reviews <- sd(listings_clean$number_of_reviews, na.rm = TRUE)
std_dev_ratings <- sd(listings_clean$review_scores_rating, na.rm = TRUE)
cat("Number Reviews Standard Deviation: ", std_dev_reviews, " ||  Rating Standard Deviation: ", std_dev_ratings, "\n")

library('stargazer')
stargazer(as.data.frame(listings_clean), type = "text", title = "Seattle Airbnb Dataset Summary Stats", style = "default")
```

## Plots
```{r}
library(ggplot2)

# scatter plot of accommodates by price 
ggplot(listings_clean) + aes(x = accommodates , y = price, color = property_type) + geom_point() + 
  theme( axis.text.y = element_blank()) + ylab("price, max = $1000")
 
# room type and accommodates
ggplot(listings_clean) + aes(x = room_type, y = accommodates) + geom_point()

# Correlation matrix
cormat <- cor(listings_clean %>% select_if(is.numeric) %>% drop_na())
corrplot::corrplot(cormat)

# neighborhood bar chart
ggplot(data = listings_clean, aes(x = neighbourhood_cleansed)) + geom_bar(fill = "blue") + coord_flip()

# Gold is Apartment, Blue is House. First 2 are Entire Home, middle 2 are private room, final 2 are shared room
boxplot(price ~ property_type + room_type, 
        data = listings_clean %>% filter(price < 600 & (property_type=="House" | property_type=="Apartment")), 
        notch=TRUE,
        col=(c("gold","blue")),
        main="Nightly Price of Airbnbs in Seattle", 
        ylab="Nightly Price",
        xlab="Property Type"
        )

# Proportion of Room Type In Each Zipcode
 ggplot(listings_clean) + geom_histogram(aes(zipcode, fill = room_type), stat = "count",alpha = 0.85, position = 'fill') + 
   theme_minimal(base_size=13) + xlab("") + ylab("")  + 
   ggtitle("The Proportion of Room Type in Each Area")
```

## Plot: Word Cloud
```{r}
# This model is a word graph showing the most commonly used words in listing names
# This method was inspired by http://www.sthda.com/english/wiki/text-mining-and-word-cloud-fundamentals-in-r-5-simple-steps-you-should-know
library("tm")
library("SnowballC")
library("wordcloud")
library("RColorBrewer")
library('wordcloud2')

docs <- Corpus(VectorSource(listings_clean$name))
toSpace <- content_transformer(function (x , pattern ) gsub(pattern, " ", x))
docs <- tm_map(docs, toSpace, "/")
docs <- tm_map(docs, toSpace, "@")
docs <- tm_map(docs, toSpace, "\\|")

# Convert the text to lower case
docs <- tm_map(docs, content_transformer(tolower))
# Remove numbers
docs <- tm_map(docs, removeNumbers)
# Remove english common stopwords
docs <- tm_map(docs, removeWords, stopwords("english"))
# Remove your own stop word
# specify your stopwords as a character vector
docs <- tm_map(docs, removeWords, c("blabla1", "blabla2")) 
# Remove punctuations
docs <- tm_map(docs, removePunctuation)
# Eliminate extra white spaces
docs <- tm_map(docs, stripWhitespace)

dtm <- TermDocumentMatrix(docs)
m <- as.matrix(dtm)
v <- sort(rowSums(m),decreasing=TRUE)
d <- data.frame(word = names(v),freq=v)
head(d, 10)

wordcloud(words = d$word, freq = d$freq, min.freq = 1,
          max.words=200, random.order=FALSE, rot.per=0.35, 
          colors=brewer.pal(8, "Dark2"))

barplot(d[1:10,]$freq, las = 2, names.arg = d[1:10,]$word,
        col ="lightblue", main ="Most frequent words",
        ylab = "Word frequencies")
```

## Train / Test Split
```{r}
train_idx <- sample(1:nrow(listings_clean),
                    size = .75 * nrow(listings_clean))

listings_train <- listings_clean %>% slice(train_idx)
listings_test <- listings_clean %>% slice(-train_idx)
```

## Model 1: Lasso Model
```{r}
# Model number one is a lasso model which will help us determine which variables are the most important
library(glmnet)
library(glmnetUtils)

lasso_mod <- cv.glmnet(price ~ host_listings_count + host_has_profile_pic + host_is_superhost + 
                           neighbourhood_cleansed + property_type + room_type + minimum_nights +
                           accommodates + bathrooms + bedrooms + beds + square_feet + number_of_reviews + 
                           review_scores_rating + cancellation_policy,
                       data = listings_train,
                       alpha = 1,
                       nfolds = 10) 
summary(lasso_mod)
plot(lasso_mod)

library('coefplot')
coef(lasso_mod, s = "lambda.min")
coef(lasso_mod, s = "lambda.1se")
```

## Model 2: Random Forest
```{r}
library('randomForest')
library('ranger')

rf_mods <- list()
oob_err <- NULL

for(i in 1:15) {
  #fit RF model and vary mtry everytime
  rf_mod <- randomForest(price ~ host_listings_count + host_has_profile_pic + host_is_superhost + 
                           neighbourhood_cleansed + property_type + room_type + minimum_nights +
                           accommodates + bathrooms + bedrooms + beds + square_feet + number_of_reviews + 
                           review_scores_rating + cancellation_policy,
                         data = listings_train,
                         mtry = i,
                         ntree = 1000)
  rf_mods[[i]] <- rf_mod
  oob_err[i] <- rf_mod$err.rate[1000]
}

results_DF <- data.frame(
  mtry = 1:9,
  oob_err
)

ggplot(results_DF, aes(x= mtry, y = oob_err)) + geom_point()

mtry_optimal <- oob_err[] # add best

rt_fit <- randomForest(price ~ .,
                       data = listings_train,
                       na.action = na.omit,
                       mtry = mtry_optimal,
                       ntree = 10000,
                       importance = TRUE,
                       localImp = TRUE) 

plot(rt_fit, ylim = c(0,1))

library('randomForestExplainer')
explain_forest(rt_fit,
               interactions = TRUE, 
               data = listings_train)
```

## Model 3: Sentiment Analysis
```{r}
#inspired by: https://www.datacamp.com/community/tutorials/sentiment-analysis-R
library(dplyr) 
library(ggplot2) 
library(gridExtra) 
library(tidytext) 
library(wordcloud2) 
library(widyr) 

library(ggplot2) 
library(ggrepel) 
library(gridExtra) 
library(knitr) 
library(kableExtra) 
library(formattable) 
library(circlize) 
library(memery) 
library(magick) 
library(yarrr)  
library(radarchart) 
library(igraph) 
library(ggraph) 

sentiment_orig <- read.csv("listings_sentiment.csv", stringsAsFactors = FALSE)
names(sentiment_orig)
head(sentiment_orig)

sentiment <- sentiment_orig %>% select(id, summary, price)
head(sentiment)

glimpse(sentiment[139,])
dim(sentiment)
str(sentiment[139, ]$summary, nchar.max = 300)

fix.contractions <- function(doc) {
  doc <- gsub("won't", "will not", doc)
  doc <- gsub("can't", "can not", doc)
  doc <- gsub("n't", " not", doc)
  doc <- gsub("'ll", " will", doc)
  doc <- gsub("'re", " are", doc)
  doc <- gsub("'ve", " have", doc)
  doc <- gsub("'m", " am", doc)
  doc <- gsub("'d", " would", doc)
  doc <- gsub("'s", "", doc)
  return(doc)
}
sentiment$summary <- sapply(sentiment$summary, fix.contractions)
head(sentiment)

removeSpecialChars <- function(x) gsub("[^a-zA-Z0-9 ]", " ", x)
sentiment$summary <- sapply(sentiment$summary, removeSpecialChars)
sentiment$price <- as.numeric(gsub("\\$", "", sentiment$price))

sentiment$summary <- sapply(sentiment$summary, tolower)

str(sentiment[139, ]$summary, nchar.max = 500)
head(sentiment)

summary(sentiment)

sentiment <- sentiment %>% mutate(pricePoint = 
           ifelse(sentiment$price %in% 0.0:100.0, "$0-$100", 
           ifelse(sentiment$price %in% 101.0:200.0, "$101-$200", 
           ifelse(sentiment$price %in% 201.0:300.0, "$201-$300", 
           ifelse(sentiment$price %in% 301.0:400.0, "$301-$400", 
           ifelse(sentiment$price %in% 401.0:500.0, "$401-$500",
           ifelse(sentiment$price %in% 501.0:1000.0, "$500-$1000",
                  "NA")))))))
head(sentiment)

my_colors <- c("#E69F00", "#56B4E9", "#009E73", "#CC79A7", "#D55E00")

theme_summary <- function() 
{
  theme(plot.title = element_text(hjust = 0.5),
        axis.text.x = element_blank(), 
        axis.ticks = element_blank(),
        panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(),
        legend.position = "none")
}

sentiment %>% filter(pricePoint != "NA") %>% group_by(pricePoint) %>% summarise(numListings = n()) %>%
  ggplot() + geom_bar(aes(x = pricePoint, y = num_listing), stat = "identity")  +
  theme(plot.title = element_text(hjust = 0.5), legend.title = element_blank(), 
  panel.grid.minor = element_blank()) +  ggtitle("Released Songs") +
  labs(x = NULL, y = "Song Count")

sentiment_words_filtered <- sentiment %>%
  unnest_tokens(word, summary) %>%
  anti_join(stop_words) %>%
  distinct() %>%
  filter(nchar(word) > 3)

class(sentiment_words_filtered)
dim(sentiment_words_filtered)

full_word_count <- sentiment %>%
  unnest_tokens(word, summary) %>%
  group_by(id,pricePoint) %>%
  summarise(num_words = n()) %>%
  arrange(desc(num_words)) 

install.packages("kableExtra")
library(kableExtra)
library(formattable)
full_word_count[1:10,] %>%
  ungroup(num_words, id) %>%
  mutate(num_words = color_bar("lightblue")(num_words)) %>%
  mutate(id = color_tile("lightpink","lightpink")(id)) %>%
  kable("html", escape = FALSE, align = "c", caption = "Summaries With Highest Word Count") %>%
  kable_styling(bootstrap_options = 
                  c("striped", "condensed", "bordered"), 
                  full_width = FALSE)

sentiment_words_filtered %>%
  count(word, sort = TRUE) %>%
  top_n(10) %>%
  ungroup() %>%
  mutate(word = reorder(word, n)) %>%
  ggplot() +
    geom_col(aes(word, n), fill = my_colors[4]) +
    theme(legend.position = "none", 
          plot.title = element_text(hjust = 0.5),
          panel.grid.major = element_blank()) +
    xlab("") + 
    ylab("id Count") +
    ggtitle("Most Frequently Used Words in Airbnb Summaries") +
    coord_flip()

sentiment_words_counts <- sentiment_words_filtered %>%
  count(word, sort = TRUE) 

#word maps of most frequent words
wordcloud2(sentiment_words_counts[1:300, ], size = .5)
letterCloud(sentiment_words_counts[1:300, ], word = "SEATTLE", size = 2)

#Words by price point
timeless_words <- sentiment_words_filtered %>% 
  filter(pricePoint != 'NA') %>%
  group_by(pricePoint) %>%
  count(word, pricePoint, sort = TRUE) %>%
  slice(seq_len(8)) %>%
  ungroup() %>%
  arrange(pricePoint,n) %>%
  mutate(row = row_number()) 

timeless_words %>%
  ggplot(aes(row, n, fill = pricePoint)) +
    geom_col(show.legend = NULL) +
    labs(x = NULL, y = "Listing Count") +
    ggtitle("Words by Price Point") + 
    theme_summary() +  
    facet_wrap(~pricePoint, scales = "free", ncol = 5) +
    scale_x_continuous(  
      breaks = timeless_words$row, 
      labels = timeless_words$word) +
    coord_flip()

#word length distribution
sentiment_word_lengths <- sentiment %>%
  unnest_tokens(word, summary) %>%
  group_by(id,pricePoint) %>%
  distinct() %>%
  mutate(word_length = nchar(word)) 

sentiment_word_lengths %>%
  count(word_length, sort = TRUE) %>%
  ggplot(aes(word_length), 
         binwidth = 10) + 
    geom_histogram(aes(fill = ..count..),
                   breaks = seq(1,25, by = 2), 
                   show.legend = FALSE) + 
    xlab("Word Length") + 
    ylab("Word Count") +
    ggtitle("Word Length Distribution") +
    theme(plot.title = element_text(hjust = 0.5),
          panel.grid.minor = element_blank())

#lexical diversity
lex_diversity_per_price <- sentiment %>%
  filter(pricePoint != "NA") %>%
  unnest_tokens(word, summary) %>%
  group_by(id,price) %>%
  summarise(lex_diversity = n_distinct(word)) %>%
  arrange(desc(lex_diversity)) 

diversity_plot <- lex_diversity_per_price %>%
  ggplot(aes(price, lex_diversity)) +
    geom_point(color = my_colors[3],
               alpha = .4, 
               size = 4, 
               position = "jitter") + 
    stat_smooth(color = "black", se = FALSE, method = "lm") +
    geom_smooth(aes(x = price, y = lex_diversity), se = FALSE,
                color = "blue", lwd = 2) +
    ggtitle("Lexical Diversity") +
    xlab("") + 
    ylab("") +
    scale_color_manual(values = my_colors) +
    theme_classic() + 
    theme_summary()

diversity_plot #doesn't really show us much, summary word diversity has a very slight decrease the higher the price

#TF-IDF (term frequency-inverse document frequency)
#only considers words that are less frequent and mean more in the analysis 
popular_tfidf_words <- sentiment %>%
  unnest_tokens(word, summary) %>%
  distinct() %>%
  filter(nchar(word) > 3) %>%
  count(pricePoint, word, sort = TRUE) %>%
  ungroup() %>%
  bind_tf_idf(word, pricePoint, n)

head(popular_tfidf_words)

top_popular_tfidf_words <- popular_tfidf_words %>%
  arrange(desc(tf_idf)) %>%
  mutate(word = factor(word, levels = rev(unique(word)))) %>%
  group_by(pricePoint) %>% 
  slice(seq_len(8)) %>%
  ungroup() %>%
  arrange(pricePoint, tf_idf) %>%
  mutate(row = row_number())

top_popular_tfidf_words %>%
  ggplot(aes(x = row, tf_idf, 
             fill = pricePoint)) +
    geom_col(show.legend = NULL) +
    labs(x = NULL, y = "TF-IDF") + 
    ggtitle("Important Words using TF-IDF by Price Point") +
    theme_summary() +  
    facet_wrap(~pricePoint, ncol = 3, scales = "free") +
    scale_x_continuous(  
      breaks = top_popular_tfidf_words$row, 
      labels = top_popular_tfidf_words$word) +
    coord_flip()

#tidy text format 
sentiment_tidy <- sentiment %>%
  unnest_tokens(word, summary) %>% 
  filter(!nchar(word) < 3) %>% 
  anti_join(stop_words) 

glimpse(sentiment_tidy)

#lexical diversity per price point
word_summary <- sentiment_tidy %>%
  mutate(pricePoint = ifelse(is.na(pricePoint),"NONE", pricePoint)) %>%
  group_by(pricePoint, id) %>%
  mutate(word_count = n_distinct(word)) %>%
  select(id, pricePoint, word_count) %>%
  distinct() %>% 
  ungroup()

pirateplot(formula =  word_count ~ pricePoint, 
   data = word_summary, 
   xlab = NULL, ylab = "Summary Distinct Word Count", 
   main = "Lexical Diversity Per Price Point", 
   pal = "google", 
   point.o = .2, 
   avg.line.o = 1, 
   theme = 0, 
   point.pch = 16, 
   point.cex = 1.5, 
   jitter.val = .1, 
   cex.lab = .9, cex.names = .7) 



install.packages("tidyr",dependencies = TRUE, repos = "http://cran.us.r-project.org")
library(dplyr) 
library(tidytext) 
library(tidyr) 
library(widyr) 
library(formattable)
library(kableExtra)
library(knitr)

my_kable_styling <- function(dat, caption) {
  kable(dat, "html", escape = FALSE, caption = caption) %>%
  kable_styling(bootstrap_options = c("striped", "condensed", "bordered"),
                full_width = FALSE)
}

#sentiment lexicons
new_sentiments <- get_sentiments("afinn")
names(new_sentiments)[names(new_sentiments) == 'value'] <- 'score'
new_sentiments <- new_sentiments %>% mutate(lexicon = "afinn", sentiment = ifelse(score >= 0, "positive", "negative"),
                                                     words_in_lexicon = n_distinct((word)))

new_sentiments %>% 
     group_by(lexicon, sentiment, words_in_lexicon) %>% 
     summarise(distinct_words = n_distinct(word)) %>% 
     ungroup() %>% 
     spread(sentiment, distinct_words) %>% 
     mutate(lexicon = color_tile("lightblue", "lightblue")(lexicon), 
            words_in_lexicon = color_bar("lightpink")(words_in_lexicon)) %>% 
     my_kable_styling(caption = "Word Counts per Lexicon")
  
new_sentiments %>%
  group_by(lexicon, sentiment, words_in_lexicon) %>%
  summarise(distinct_words = n_distinct(word)) %>%
  ungroup() %>%
  spread(sentiment, distinct_words) %>%
  mutate(lexicon = color_tile("lightblue", "lightblue")(lexicon),
         words_in_lexicon = color_bar("lightpink")(words_in_lexicon)) %>%
  my_kable_styling(caption = "Word Counts Per Lexicon")

sentiment_tidy %>%
  mutate(words_in_summary = n_distinct(word)) %>%
  inner_join(new_sentiments) %>%
  group_by(lexicon, words_in_summary, words_in_lexicon) %>%
  summarise(lex_match_words = n_distinct(word)) %>%
  ungroup() %>%
  mutate(total_match_words = sum(lex_match_words), #Not used but good to have
         match_ratio = lex_match_words / words_in_summary) %>%
  select(lexicon, lex_match_words,  words_in_summary, match_ratio) %>%
  mutate(lex_match_words = color_bar("lightpink")(lex_match_words),
         lexicon = color_tile("lightgreen", "lightgreen")(lexicon)) %>%
  my_kable_styling(caption = "Words Found In Lexicons")

new_sentiments %>%
  filter(word %in% c("dark", "controversy", "gangster",
                     "discouraged", "race")) %>%
  arrange(word) %>% #sort
  select(-score) %>% #remove this field
  mutate(word = color_tile("lightblue", "lightblue")(word),
         words_in_lexicon = color_bar("lightpink")(words_in_lexicon),
         lexicon = color_tile("lightgreen", "lightgreen")(lexicon)) %>%
  my_kable_styling(caption = "Specific Words")

sentiment_bing <- sentiment_tidy %>%
  inner_join(get_sentiments("bing"))

sentiment_nrc <- sentiment_tidy %>%
  inner_join(get_sentiments("nrc"))

sentiment_nrc_sub <- sentiment_tidy %>%
  inner_join(get_sentiments("nrc")) %>%
  filter(!sentiment %in% c("positive", "negative"))

library(ggplot2)
nrc_plot <- sentiment_nrc %>%
  group_by(sentiment) %>%
  summarise(word_count = n()) %>%
  ungroup() %>%
  mutate(sentiment = reorder(sentiment, word_count)) %>%
  #Use `fill = -word_count` to make the larger bars darker
  ggplot(aes(sentiment, word_count, fill = -word_count)) +
  geom_col() +
  guides(fill = FALSE) + #Turn off the legend
  theme_summary() +
  labs(x = NULL, y = "Word Count") +
  scale_y_continuous(limits = c(0, 15000)) + #Hard code the axis limit
  ggtitle("Listings NRC Sentiment") +
  coord_flip()

nrc_plot
```
